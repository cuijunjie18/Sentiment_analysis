{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed0e3c8",
   "metadata": {},
   "source": [
    "### **NLP应用——情感分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737ad6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from net_frame import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2c296",
   "metadata": {},
   "source": [
    "**一、数据预处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26de322f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read pos:   0%|          | 0/12500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read pos: 100%|██████████| 12500/12500 [00:00<00:00, 58309.81it/s]\n",
      "read neg: 100%|██████████| 12500/12500 [00:00<00:00, 72940.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data use time:0.4356949641369283s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm # 使用会减慢循环效率\n",
    "import time\n",
    "\n",
    "# 方法1\n",
    "def get_raw_data(data_path = 'aclImdb',istrain = True):\n",
    "    \"\"\"获取原始的sentence及标签\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    path_prefix = os.path.join(data_path,'train' if istrain else 'test')\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label in ['pos','neg']:\n",
    "        folder = os.path.join(path_prefix,label)\n",
    "\n",
    "        # 这样写比在最后加loop.set_description(f\"read {label}\")快50倍\n",
    "        loop = tqdm(os.listdir(folder),total = len(os.listdir(folder)),desc = 'read ' + label)\n",
    "        for filename in loop:\n",
    "        # for filename in os.listdir(folder):\n",
    "            filename = os.path.join(folder,filename)\n",
    "            with open(filename,'r',encoding = 'utf-8') as file:\n",
    "                for line in file:\n",
    "                    data.append(line.strip())\n",
    "                    labels.append(1 if label == 'pos' else 0)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Load data use time:{end - start}s\")\n",
    "    return data,labels\n",
    "\n",
    "# 方法2\n",
    "def get_raw_data2(data_path = 'aclImdb',istrain = True):\n",
    "    import glob\n",
    "    \"\"\"使用glob遍历文件\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    path_prefix = os.path.join(data_path,'train' if istrain else 'test')\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label in ['pos','neg']:\n",
    "        folder = os.path.join(path_prefix,label,\"*.txt\")\n",
    "        loop = tqdm(glob.glob(folder),total = len(glob.glob(folder)),desc = 'read ' + label)\n",
    "        for filename in loop:\n",
    "            with open(filename,'r',encoding = 'utf-8') as file:\n",
    "                for line in file:\n",
    "                    data.append(line.strip())\n",
    "                    labels.append(1 if label == 'pos' else 0)\n",
    "            loop.set_description(f\"read {label}\",refresh = False)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Load data use time:{end - start}s\")\n",
    "    return data,labels\n",
    "\n",
    "train_data,train_labels = get_raw_data()\n",
    "# test_data,test_labels = get_raw_data(istrain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71693c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# 检查数据是否读取完全\n",
    "print(len(train_data))\n",
    "print(len(train_labels))\n",
    "# print(len(test_data))\n",
    "# print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901a4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始数据tokenize\n",
    "source = tokenize(train_data,token = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c26108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词表(文本分类仅需<pad>为保留token)\n",
    "source_vocab = Vocab(source,reserved_tokens = ['<pad>'])\n",
    "# target_vocab = Vocab(target,reserved_tokens = ['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6a1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_array(text,vocab,num_steps):\n",
    "    \"\"\"文本token to idx,将原始文本标量化\"\"\"\n",
    "    lines = [vocab[l] for l in text]\n",
    "    arrays = [truncate_pad(line,num_steps,padding_token = vocab['<pad>']) for line in lines]\n",
    "    return torch.tensor(arrays,dtype = torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ea55dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 500])\n",
      "torch.Size([25000, 1])\n"
     ]
    }
   ],
   "source": [
    "# 数据标量化\n",
    "num_steps = 500\n",
    "source_arrays = build_array(source,source_vocab,num_steps = 500)\n",
    "target_arrays = torch.tensor(train_labels).reshape(-1,1)\n",
    "print(source_arrays.shape)\n",
    "print(target_arrays.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40ded17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据迭代器\n",
    "batch_size = 64\n",
    "train_iter = data.DataLoader(\n",
    "    data.TensorDataset(source_arrays,target_arrays),batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e522ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 500]) torch.Size([1, 1])\n",
      "tensor([[     9,    388,      6,     67, 109697,   6092,     26,   1354,     51,\n",
      "              6,    543,      4,     14,     81,   3598,   6696,     19,    139,\n",
      "            342,     30,   4299,   2688,     14,   1697,      4,      2,     77,\n",
      "            457,     14,   2785,    503,    436,   4082,   2867,    125,     23,\n",
      "            747,    514,    342,      5,    139,     15,    116,     15,   6172,\n",
      "             56,    154,     28,   1769,     10,     91,     16,   1710,    889,\n",
      "          10178,     27,   1632,      4,     11,     21,      7,      3,    468,\n",
      "            709,      5,    475,    678,   1165,     11,    469,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1]], dtype=torch.int32)\n",
      "I went to see Antone Fisher not knowing what to expect and was most pleasantly surprised. The acting job by Derek Luke was outstanding and the story line was excellent. Of course Denzel Washington did his usual fine job of acting as well as directing. It makes you realized that people with mental problems CAN be helped and this movie is a perfect example of this. Don't miss this one. <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "# 测试数据是否正确加载\n",
    "for batch in train_iter:\n",
    "    X,Y = batch\n",
    "    print(X.shape,Y.shape)\n",
    "    print(X)\n",
    "    str = \"\"\n",
    "    for x in X[0]:\n",
    "        str += source_vocab.to_tokens(x.item()) + \" \"\n",
    "    print(str)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be2242",
   "metadata": {},
   "source": [
    "**二、使用RNN架构去搭建模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b3ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "x = torch.rand((100,50))\n",
    "print(x.permute(1,0) == x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1633dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 将bidirectional设置为True以获取双向循环神经网络\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是（批量大小，时间步数）\n",
    "        # 因为长短期记忆网络要求其输入的第一个维度是时间维，\n",
    "        # 所以在获得词元表示之前，输入会被转置。\n",
    "        # 输出形状为（时间步数，批量大小，词向量维度）\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "        # 返回上一个隐藏层在不同时间步的隐状态，\n",
    "        # outputs的形状是（时间步数，批量大小，2*隐藏单元数）\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # 连结初始和最终时间步的隐状态，作为全连接层的输入，\n",
    "        # 其形状为（批量大小，4*隐藏单元数）\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed212617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权重初始化函数\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdaa32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QwenLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
